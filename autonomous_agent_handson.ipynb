{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hands on building autonomous agents\n",
        "\n",
        "This is a bare-minimal implementation of an autonomous agent that can search Arxiv for relevant papers and summarize the core ideas of the paper based on the user's research question.\n",
        "\n",
        "In the first example, we will just create a highly-abstracted version, in which LangChain does all the orchestration under the hoood.\n",
        "\n",
        "In the second example, we will dive deep into the details and build a similar autonomous agent from scratch without using LangChain."
      ],
      "metadata": {
        "id": "3QttIbUKJgcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install packages & setting up environment"
      ],
      "metadata": {
        "id": "SnlVqqHNKDNh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ShGS9PSJdji",
        "outputId": "eacde14c-d431-4d26-967d-6d31bd08a049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.154-py3-none-any.whl (709 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.9/709.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.65.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses-json<0.6.0,>=0.5.7\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting typing-inspect>=0.4.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, openapi-schema-pydantic, marshmallow-enum, aiosignal, dataclasses-json, aiohttp, langchain\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.7 frozenlist-1.3.3 langchain-0.0.154 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.8.0 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting arxiv\n",
            "  Downloading arxiv-1.4.7-py3-none-any.whl (12 kB)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=f0a716697823d79db3cc537a0da7baa07bcc5b489b19bc53a7412ada1dc41779\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-1.4.7 feedparser-6.0.10 sgmllib3k-1.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.5-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.5\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install arxiv\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-......\" # Replace the string content with your OpenAI API key"
      ],
      "metadata": {
        "id": "hlJRJ7WeKLwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A high-level abstracted example using LangChain\n",
        "\n",
        "This section is a very high level example of how to define an agent using LangChain.\n",
        "- We first define the tool that the language model can use: arxiv api, including the name and descriptions for the tool, so that the LLM knows it can use the tool when needed\n",
        "- We then initialize the agent by passing it the tools and the LLM model\n",
        "- Finall, we ask the agent a question. The agent will use the ReAct framework to complete the task. It first thinks if it has all the information needed, then decides to use the arxiv API to search for relevant papers, when it gets the papers, it will summarize the content and give them back to the user."
      ],
      "metadata": {
        "id": "F8G0T-sFKXFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary packages\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.agents import Tool\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.utilities import ArxivAPIWrapper"
      ],
      "metadata": {
        "id": "_emlbCkiKZmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0) # Initialize the LLM to be used\n",
        "\n",
        "arxiv = ArxivAPIWrapper()\n",
        "arxiv_tool = Tool(\n",
        "    name=\"arxiv_search\",\n",
        "    description=\"Search on arxiv. The tool can search a keyword on arxiv for the top papers. It will return publishing date, title, authors, and summary of the papers.\",\n",
        "    func=arxiv.run\n",
        ")\n",
        "\n",
        "tools = [arxiv_tool]\n",
        "\n",
        "agent_chain = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "agent_chain.run(\"What is ReAct reasoning and acting in language models?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OcXCmWJsMPjq",
        "outputId": "5b89b6e6-ca9d-40a1-bd4e-cd2b351d752a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI'm not sure what ReAct reasoning and acting in language models is, so I should use arxiv_search to find out more information.\n",
            "Action: arxiv_search\n",
            "Action Input: \"ReAct reasoning and acting in language models\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPublished: 2023-03-10\n",
            "Title: ReAct: Synergizing Reasoning and Acting in Language Models\n",
            "Authors: Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao\n",
            "Summary: While large language models (LLMs) have demonstrated impressive capabilities\n",
            "across tasks in language understanding and interactive decision making, their\n",
            "abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\n",
            "action plan generation) have primarily been studied as separate topics. In this\n",
            "paper, we explore the use of LLMs to generate both reasoning traces and\n",
            "task-specific actions in an interleaved manner, allowing for greater synergy\n",
            "between the two: reasoning traces help the model induce, track, and update\n",
            "action plans as well as handle exceptions, while actions allow it to interface\n",
            "with external sources, such as knowledge bases or environments, to gather\n",
            "additional information. We apply our approach, named ReAct, to a diverse set of\n",
            "language and decision making tasks and demonstrate its effectiveness over\n",
            "state-of-the-art baselines, as well as improved human interpretability and\n",
            "trustworthiness over methods without reasoning or acting components.\n",
            "Concretely, on question answering (HotpotQA) and fact verification (Fever),\n",
            "ReAct overcomes issues of hallucination and error propagation prevalent in\n",
            "chain-of-thought reasoning by interacting with a simple Wikipedia API, and\n",
            "generates human-like task-solving trajectories that are more interpretable than\n",
            "baselines without reasoning traces. On two interactive decision making\n",
            "benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\n",
            "reinforcement learning methods by an absolute success rate of 34% and 10%\n",
            "respectively, while being prompted with only one or two in-context examples.\n",
            "Project site with code: https://react-lm.github.io\n",
            "\n",
            "Published: 2023-03-20\n",
            "Title: MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action\n",
            "Authors: Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang\n",
            "Summary: We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of\n",
            "vision experts to achieve multimodal reasoning and action. In this paper, we\n",
            "define and explore a comprehensive list of advanced vision tasks that are\n",
            "intriguing to solve, but may exceed the capabilities of existing vision and\n",
            "vision-language models. To achieve such advanced visual intelligence, MM-REACT\n",
            "introduces a textual prompt design that can represent text descriptions,\n",
            "textualized spatial coordinates, and aligned file names for dense visual\n",
            "signals such as images and videos. MM-REACT's prompt design allows language\n",
            "models to accept, associate, and process multimodal information, thereby\n",
            "facilitating the synergetic combination of ChatGPT and various vision experts.\n",
            "Zero-shot experiments demonstrate MM-REACT's effectiveness in addressing the\n",
            "specified capabilities of interests and its wide application in different\n",
            "scenarios that require advanced visual understanding. Furthermore, we discuss\n",
            "and compare MM-REACT's system paradigm with an alternative approach that\n",
            "extends language models for multimodal scenarios through joint finetuning.\n",
            "Code, demo, video, and visualization are available at\n",
            "https://multimodal-react.github.io/\n",
            "\n",
            "Published: 2013-07-29\n",
            "Title: ReAct! An Interactive Tool for Hybrid Planning in Robotics\n",
            "Authors: Zeynep Dogmus, Esra Erdem, Volkan Patoglu\n",
            "Summary: We present ReAct!, an interactive tool for high-level reasoning for cognitive\n",
            "robotic applications. ReAct! enables robotic researchers to describe robots'\n",
            "actions and change in dynamic domains, without having to know about the\n",
            "syntactic and semantic details of the underlying formalism in advance, and\n",
            "solve planning problems using state-of-the-art automated reasoners, without\n",
            "having to learn about their input/output language or usage. In particular,\n",
            "ReAct! can be used to represent sophisticated dynamic domains that feature\n",
            "concurrency, indirect effects of actions, and state/transition constraints. It\n",
            "allows for embedding externally defined calculations (e.g., checking for\n",
            "collision-free continuous trajectories) into representations of hybrid domains\n",
            "that require a tight integration of (discrete) high-level reasoning with\n",
            "(continuous) geometric reasoning. ReAct! also enables users to solve planning\n",
            "problems that involve complex goals. Such variety of utilities are useful for\n",
            "robotic researchers to work on interesting and challenging domains, ranging\n",
            "from service robotics to cognitive factories. ReAct! provides sample\n",
            "formalizations of some action domains (e.g., multi-agent path planning, Tower\n",
            "of Hanoi), as well as dynamic simulations of plans computed by a\n",
            "state-of-the-art automated reasoner (e.g., a SAT solver or an ASP solver).\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mBased on the observations, ReAct reasoning and acting in language models is a technique that integrates reasoning and action generation in language models, allowing for greater synergy between the two. It has been applied to various language and decision-making tasks and has demonstrated effectiveness over state-of-the-art baselines. There are also other papers that use the term ReAct in different contexts, such as a tool for high-level reasoning for cognitive robotic applications and a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. \n",
            "Final Answer: ReAct reasoning and acting in language models is a technique that integrates reasoning and action generation in language models, allowing for greater synergy between the two. It has been applied to various language and decision-making tasks and has demonstrated effectiveness over state-of-the-art baselines.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ReAct reasoning and acting in language models is a technique that integrates reasoning and action generation in language models, allowing for greater synergy between the two. It has been applied to various language and decision-making tasks and has demonstrated effectiveness over state-of-the-art baselines.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building an autonomous agent from scratch\n",
        "\n",
        "In this example, we will create an autonomous agent from scratch. The goal of this part is for you to understand how the components of autonomous agents work, instead of showing the best implementation for production. For production purposes, I recommend using pre-packages framworks like LangChain to get the job done faster.\n"
      ],
      "metadata": {
        "id": "ohDcigCnYKY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install arxiv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C08NsH1qYmSn",
        "outputId": "5b451263-fdc5-44f8-9222-379502935de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting arxiv\n",
            "  Downloading arxiv-1.4.7-py3-none-any.whl (12 kB)\n",
            "Collecting feedparser (from arxiv)\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sgmllib3k (from feedparser->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=21571cc6f771b2e48a06b77dc56989ddd4372ec00f037ca3daa399f49c548f4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-1.4.7 feedparser-6.0.10 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import arxiv\n",
        "\n",
        "# Set up the OpenAI API\n",
        "openai.api_key = \"sk-......\" # Replace the string content with your OpenAI API key\n",
        "\n",
        "\"\"\"\n",
        "Wrap the OpenAI API call in this function\n",
        "\"\"\"\n",
        "def getResponse(prompt):\n",
        "    response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            temperature = 0, # We want consistent behavior, so we set a very low temperature\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You're a helpful assistant. Carefully follow the user's instructions.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "    response = str(response['choices'][0]['message']['content'])\n",
        "    return response\n",
        "\n",
        "\"\"\"\n",
        "Use GPT to determine the action to take by giving it the objective, memory, and tools.\n",
        "If it think it has finished the objective, just give the answer.\n",
        "If it needs more info, it will pick the tool to get the relevant information based on the tool description.\n",
        "\"\"\"\n",
        "def determineAction(objective, memory, tools):\n",
        "    formattedPrompt = f\"\"\"Determine if the following memory is enough to answer\\n\n",
        "    the user's objective. Your past actions are stored in the memory for reference\\n\n",
        "    If it is enough, answer the question in the format: 'FINAL ANSWER: <your answer>'. \\n\n",
        "    If the memory is not enough, you can use a tool in the available tools section\\n\n",
        "    to get more information. When using a tool you should use this format: \\n\n",
        "    'USE <tool name>:<parameter>'. If no tool can help you achieve the user's \\n\n",
        "    objective, then answer 'FINAL: CANNOT ANSWER'.\n",
        "\n",
        "    ```Objective\n",
        "    Answer: {objective}\n",
        "    ```\n",
        "\n",
        "    ```Memory\n",
        "    {memory}\n",
        "    ```\n",
        "\n",
        "    ```Available Tools\n",
        "    {tools}\n",
        "    ```\n",
        "\n",
        "    \"\"\"\n",
        "    response = getResponse(formattedPrompt)\n",
        "    (finished, result, memory) = parseResponse(response, memory,tools)\n",
        "    return (finished, result, memory)\n",
        "\n",
        "\"\"\"\n",
        "Parse the response from GPT to determine if the objective is finished.\n",
        "If it is finished, just give the final answer.\n",
        "If the objective cannot be finished with the context and tools, it will say it cannot answer\n",
        "If GPT picks a tool, execute the tool and save the result of the tool in memory.\n",
        "\"\"\"\n",
        "def parseResponse(response, memory,tools):\n",
        "    finished = False\n",
        "\n",
        "    if response.startswith('FINAL ANSWER:'):\n",
        "        finished = True\n",
        "        memory.append(response)\n",
        "        return (finished, response, memory)\n",
        "    elif response == 'FINAL: CANNOT ANSWER':\n",
        "        finished = True\n",
        "        memory.append(response)\n",
        "        return (finished, response, memory)\n",
        "    elif response.startswith('USE '):\n",
        "        # split the string using ':' as the delimiter\n",
        "        parsed_str = response.split(':')\n",
        "\n",
        "        # get the tool name and parameter\n",
        "        tool_name = parsed_str[0].split()[1]\n",
        "        parameter = parsed_str[1]\n",
        "\n",
        "        print(\"THOUGHT: \" + response)\n",
        "        memory.append(\"THOUGHT: \" + response)\n",
        "\n",
        "        result = executeTool(tool_name, parameter,tools)\n",
        "\n",
        "        new_memory = \"OBSERVATION: \" + str(result)\n",
        "        print(new_memory)\n",
        "        memory.append(new_memory)\n",
        "\n",
        "        return (finished, result, memory)\n",
        "\n",
        "\"\"\"\n",
        "Execute the tool that GPT picks using the parameter it gives.\n",
        "Returns the execution result so that GPT can have the relevant info.\n",
        "\"\"\"\n",
        "def executeTool(tool_name, parameter,tools):\n",
        "    # Find the tool with the given name\n",
        "    tool = None\n",
        "    for t in tools:\n",
        "        if t['tool_name'] == tool_name:\n",
        "            tool = t\n",
        "            break\n",
        "    \n",
        "    # If the tool is found, execute its function with the given parameter\n",
        "    if tool:\n",
        "        return tool['function_name'](parameter)\n",
        "    else:\n",
        "        return \"Tool not found\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Wrap the search arxiv function as a tool for GPT\n",
        "Input is a search keyword\n",
        "Output is a list of dictionaries with title, published date, authors, and summary of papers\n",
        "\"\"\"\n",
        "def searchArxiv(keyword):\n",
        "    # Perform a search with the given query\n",
        "    search = arxiv.Search(query=keyword, max_results=3)\n",
        "    \n",
        "    # Get the metadata for each result and extract relevant information\n",
        "    results = []\n",
        "    for result in search.results():\n",
        "        title = result.title\n",
        "        published_date = result.published.strftime(\"%Y-%m-%d\")\n",
        "        authors = \", \".join(author.name for author in result.authors)\n",
        "        summary = result.summary\n",
        "        \n",
        "        # Store the extracted information as a dictionary\n",
        "        results.append((\n",
        "            \"title: \" + title,\n",
        "            \"published_date: \" + published_date,\n",
        "            \"authors: \" + authors,\n",
        "            \"summary: \" + summary\n",
        "        ))\n",
        "    \n",
        "    # Return the list of tuples containing the result information\n",
        "    return results\n",
        "\n",
        "\"\"\"\n",
        "Initialize memory, tools for the GPT agent.\n",
        "Ask for a user objective and let it run iteratively untill the objective is achieved.\n",
        "As a safety measure, it will also stop after 5 iterations just in case things go wrong.\n",
        "\"\"\"\n",
        "def startAgent():\n",
        "    objective = input(\"What is your research question? \")\n",
        "    # For simplicity, we will just use a list to store every thing. \n",
        "    # For production, you will probably use vector databases.\n",
        "    memory = []\n",
        "\n",
        "    tools = [{'tool_name': 'searchArxiv', \n",
        "            'description': \"\"\"You can use this tool to search for scientific papers on Arxiv. The response will have title, author, published date, and summary.\"\"\", \n",
        "            'function_name' : searchArxiv,\n",
        "            'parameter': 'search key word'}]\n",
        "    \n",
        "    n = 0\n",
        "    while True:\n",
        "        (finished, result, memory) = determineAction(objective, memory, tools)\n",
        "        n += 1\n",
        "\n",
        "        if finished:\n",
        "            print(result)\n",
        "            return\n",
        "        \n",
        "        if n > 5:\n",
        "            print(\"Ended for reaching limit.\")\n",
        "            return\n",
        "\n",
        "\n",
        "startAgent()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3--ub94ffTb9",
        "outputId": "8681eced-93ad-4551-cda9-b50395a0f61b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is your research question? What is ReAct reasoning and acting in language models?\n",
            "THOUGHT: USE searchArxiv:ReAct reasoning and acting in language models.\n",
            "OBSERVATION: [('title: ReAct: Synergizing Reasoning and Acting in Language Models', 'published_date: 2022-10-06', 'authors: Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao', 'summary: While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io'), ('title: MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action', 'published_date: 2023-03-20', 'authors: Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang', \"summary: We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of\\nvision experts to achieve multimodal reasoning and action. In this paper, we\\ndefine and explore a comprehensive list of advanced vision tasks that are\\nintriguing to solve, but may exceed the capabilities of existing vision and\\nvision-language models. To achieve such advanced visual intelligence, MM-REACT\\nintroduces a textual prompt design that can represent text descriptions,\\ntextualized spatial coordinates, and aligned file names for dense visual\\nsignals such as images and videos. MM-REACT's prompt design allows language\\nmodels to accept, associate, and process multimodal information, thereby\\nfacilitating the synergetic combination of ChatGPT and various vision experts.\\nZero-shot experiments demonstrate MM-REACT's effectiveness in addressing the\\nspecified capabilities of interests and its wide application in different\\nscenarios that require advanced visual understanding. Furthermore, we discuss\\nand compare MM-REACT's system paradigm with an alternative approach that\\nextends language models for multimodal scenarios through joint finetuning.\\nCode, demo, video, and visualization are available at\\nhttps://multimodal-react.github.io/\"), ('title: ReAct! An Interactive Tool for Hybrid Planning in Robotics', 'published_date: 2013-07-29', 'authors: Zeynep Dogmus, Esra Erdem, Volkan Patoglu', \"summary: We present ReAct!, an interactive tool for high-level reasoning for cognitive\\nrobotic applications. ReAct! enables robotic researchers to describe robots'\\nactions and change in dynamic domains, without having to know about the\\nsyntactic and semantic details of the underlying formalism in advance, and\\nsolve planning problems using state-of-the-art automated reasoners, without\\nhaving to learn about their input/output language or usage. In particular,\\nReAct! can be used to represent sophisticated dynamic domains that feature\\nconcurrency, indirect effects of actions, and state/transition constraints. It\\nallows for embedding externally defined calculations (e.g., checking for\\ncollision-free continuous trajectories) into representations of hybrid domains\\nthat require a tight integration of (discrete) high-level reasoning with\\n(continuous) geometric reasoning. ReAct! also enables users to solve planning\\nproblems that involve complex goals. Such variety of utilities are useful for\\nrobotic researchers to work on interesting and challenging domains, ranging\\nfrom service robotics to cognitive factories. ReAct! provides sample\\nformalizations of some action domains (e.g., multi-agent path planning, Tower\\nof Hanoi), as well as dynamic simulations of plans computed by a\\nstate-of-the-art automated reasoner (e.g., a SAT solver or an ASP solver).\")]\n",
            "FINAL ANSWER: ReAct reasoning and acting in language models is a technique that allows large language models to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two. It has been shown to be effective in a diverse set of language and decision-making tasks and outperforms state-of-the-art baselines.\n"
          ]
        }
      ]
    }
  ]
}